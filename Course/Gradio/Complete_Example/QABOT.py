from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames
from ibm_watsonx_ai import Credentials
from langchain_ibm import WatsonxLLM, WatsonxEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA

import gradio as gr

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


## LLM
def get_llm():
    model_id = 'ibm/granite-3-3-8b-instruct'
    parameters = {
        GenParams.MAX_NEW_TOKENS: 256,
        GenParams.TEMPERATURE : 0.5,
    }
    watsonx_llm = WatsonxLLM(
        model_id=model_id,
        url="https://us-south.ml.cloud.ibm.com",
        project_id="skills-network",
        params= parameters,
    )
    return watsonx_llm

def document_loader(file):
    loader = PyPDFLoader(file.name)
    loaded_document = loader.load()
    return loaded_document

def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # Increased from 500 for better context
        chunk_overlap=100, # Increased overlap for better continuity
        length_function=len,
    )
    chunks = text_splitter.split_documents(data)
    
    # Debug: Print first few chunks to see content
    print(f"=== TEXT SPLITTER DEBUG ===")
    print(f"Total chunks created: {len(chunks)}")
    for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks
        print(f"\n--- Chunk {i+1} ---")
        print(f"Page: {chunk.metadata.get('page', 'unknown')}")
        print(f"Content (first 200 chars): {chunk.page_content[:200]}...")
    print("=== END TEXT SPLITTER DEBUG ===\n")
    
    return chunks

## Embedding model
def watsonx_embedding():
    embed_params = {
        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 512,
        EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True}
    }
    
    watsonx_embedding = WatsonxEmbeddings(
        model_id="ibm/slate-125m-english-rtrvr",
        url="https://us-south.ml.cloud.ibm.com",
        project_id="skills-network",
        params=embed_params,
    )
    return watsonx_embedding

## Vector db
def vector_database(chunks):
    try:
        embedding_model = watsonx_embedding()
        
        # Filter out empty or very short chunks
        valid_chunks = []
        for chunk in chunks:
            if hasattr(chunk, 'page_content') and chunk.page_content.strip():
                # Only keep chunks with meaningful content (at least 10 characters)
                if len(chunk.page_content.strip()) >= 10:
                    valid_chunks.append(chunk)
        
        if not valid_chunks:
            raise ValueError("No valid text chunks found in the PDF")
        
        print(f"Processing {len(valid_chunks)} text chunks...")
        
        # Create vector database with filtered chunks
        vectordb = Chroma.from_documents(valid_chunks, embedding_model)
        return vectordb
        
    except Exception as e:
        print(f"Error in vector_database: {str(e)}")
        raise Exception(f"Failed to create vector database: {str(e)}")

## Retriever
def retriever(file):
    splits = document_loader(file)
    chunks = text_splitter(splits)
    vectordb = vector_database(chunks)
    # Increase k to retrieve more relevant chunks
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})  # Retrieve top 5 chunks instead of default
    return retriever

## QA Chain
def retriever_qa(file, query):
    try:
        if not file:
            return "‚ùå Please upload a PDF file first."
        
        if not query.strip():
            return "‚ùå Please enter a question."
        
        print(f"Processing file: {file.name}")
        print(f"Question: {query}")
        
        llm = get_llm()
        retriever_obj = retriever(file)
        
        # Create a more specific prompt template
        from langchain.prompts import PromptTemplate
        
        template = """Use only the following context from the PDF document to answer the question. 
        Do not reference any code, programming, or application interfaces. 
        Focus only on the actual content and information within the PDF document.

        Context from PDF:
        {context}

        Question: {question}

        Answer based only on the PDF content:"""
        
        PROMPT = PromptTemplate(
            template=template, input_variables=["context", "question"]
        )
        
        qa = RetrievalQA.from_chain_type(
            llm=llm, 
            chain_type="stuff", 
            retriever=retriever_obj, 
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        response = qa.invoke(query)
        
        # Debug: Print retrieved sources
        if 'source_documents' in response:
            print(f"Retrieved {len(response['source_documents'])} source documents")
            print("=== DEBUGGING RETRIEVED CONTENT ===")
            for i, doc in enumerate(response['source_documents']):
                print(f"\n--- Source {i+1} (Page {doc.metadata.get('page', 'unknown')}) ---")
                print(f"Content preview: {doc.page_content[:300]}...")
                print(f"Full content length: {len(doc.page_content)} characters")
            print("=== END DEBUG ===\n")
        
        return f"‚úÖ Answer: {response['result']}"
        
    except Exception as e:
        error_msg = f"‚ùå Error processing your request: {str(e)}"
        print(error_msg)
        return f"{error_msg}\n\nüí° Suggestions:\n‚Ä¢ Try uploading a smaller PDF file\n‚Ä¢ Make sure the PDF contains readable text\n‚Ä¢ Ask a simpler question\n‚Ä¢ Check if the PDF is text-based (not just images)"


# Create Gradio interface
rag_application = gr.Interface(
    fn=retriever_qa,
    allow_flagging="never",
    inputs=[
        gr.File(label="Upload PDF File", file_count="single", file_types=['.pdf'], type="filepath"),  # Drag and drop file upload
        gr.Textbox(label="Input Query", lines=2, placeholder="Type your question here...")
    ],
    outputs=gr.Textbox(label="Output"),
    title="Roja ai chatbot",
    description="Upload a PDF document and ask any question. The chatbot will try to answer using the provided document."
)

# Launch the app
rag_application.launch(server_name="127.0.0.1", server_port=7862)